{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinakeshvari/Alcoholic_CNN_Classification/blob/main/Project02_DS04_S02_WordEmbedding_RezaShokrzad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“° News Article Similarity Analysis using NLP ğŸ§ ğŸ“Š\n",
        "\n",
        "## ğŸ¯ Objective\n",
        "This notebook explores text similarity among news articles using NLP techniques. We will:\n",
        "\n",
        "âœ… Convert text into numerical vectors using pre-trained **GloVe embeddings**.  \n",
        "âœ… Reduce dimensionality for visualization using **PCA**.  \n",
        "âœ… Apply **K-Means clustering** to group similar news articles.  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ” Why GloVe?\n",
        "GloVe (**Global Vectors for Word Representation**) provides **dense vector embeddings** trained on a large corpus, allowing us to capture the **semantic meaning** of words efficiently. ğŸ†âœ¨  \n",
        "\n",
        "Using GloVe, we can **transform text into meaningful numerical representations** for further processing and clustering. ğŸš€ğŸ“–\n"
      ],
      "metadata": {
        "id": "81F8Yu38bxwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“¥ Import necessary libraries\n",
        "# â¬‡ï¸ Download stopwords for text cleaning\n",
        "\n"
      ],
      "metadata": {
        "id": "TMmGkOAkbxky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“Š Data Overview\n",
        "The dataset consists of news articles with the following columns:\n",
        "\n",
        "ğŸ“° **title**: The headline of the news article.  \n",
        "ğŸ“„ **content**: The full text of the article.  \n",
        "\n",
        "ğŸ¯ Our goal is to **analyze the similarity between articles** and **group them into clusters** for better understanding. ğŸ”ğŸ¤–  \n"
      ],
      "metadata": {
        "id": "hx4OHOf2b3lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# download the dataset\n",
        "!wget https://raw.githubusercontent.com/mage-ai/datasets/refs/heads/master/news_articles.csv\n"
      ],
      "metadata": {
        "id": "Iji0XRKmhHB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load dataset\n",
        "\n",
        "# Display basic dataset information\n"
      ],
      "metadata": {
        "id": "mWB47xZzehTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ› ï¸ Text Preprocessing\n",
        "âœ… **Lowercasing** to standardize words. ğŸ”¡  \n",
        "âœ… **Removing special characters and punctuation** to clean the text. âœ‚ï¸ğŸ§¹  \n"
      ],
      "metadata": {
        "id": "G9HNIBxhcCsx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "# Handling missing values\n",
        "\n",
        "# Text Cleaning Function\n",
        "# Lowercasing\n",
        "# Remove non-word characters\n",
        "\n",
        "\n",
        "\n",
        "#apply the above function on the dataframe\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ† GloVe-Based Embeddings  \n",
        "Each article is converted into a **100-dimensional numerical vector** using **GloVe embeddings**. ğŸ”¢âœ¨  \n",
        "\n",
        "ğŸ”¹ If a word is **found** in GloVe, its corresponding **vector** is used. âœ…  \n",
        "ğŸ”¹ If **no words** in an article match the GloVe vocabulary, a **zero vector** is assigned. âš ï¸0ï¸âƒ£  \n",
        "\n",
        "Using these vectors, we can numerically represent text for further **analysis and clustering**. ğŸ“ŠğŸ”ğŸš€  \n"
      ],
      "metadata": {
        "id": "J6pHTsaecJO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load GloVe Embeddings\n",
        "glove_path = \"glove/glove.6B.100d.txt\"  # Change if using a different version\n"
      ],
      "metadata": {
        "id": "UfWCxxQ5cEDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert articles to vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "5z59rGsrcHnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## â„¹ï¸ Understanding PCA for Dimensionality Reduction  \n",
        "\n",
        "### Why PCA? ğŸ¤”  \n",
        "When working with **high-dimensional data**, itâ€™s difficult to visualize and analyze patterns. **Principal Component Analysis (PCA)** helps by reducing the number of dimensions while **preserving important information**.  \n",
        "\n",
        "### How Does It Work? âš™ï¸  \n",
        "PCA transforms our **high-dimensional word embeddings** (100 dimensions) into **2 principal components** that capture most of the variance in the data. This allows us to **visualize articles in a 2D space**, making clustering more interpretable.  \n",
        "\n",
        "ğŸ¯ **Goal:** Reduce dimensionality while retaining the most important information for better visualization and clustering.  \n",
        "ğŸ“Š **Next Step:** Weâ€™ll plot the articles in a 2D space to see if meaningful patterns emerge! ğŸš€  \n"
      ],
      "metadata": {
        "id": "XHeatmrZetsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce to 2D for visualization\n",
        "\n",
        "\n",
        "# Store in DataFrame\n",
        "df['pca_x'] = X_pca[:, 0]\n",
        "df['pca_y'] = X_pca[:, 1]\n",
        "\n",
        "# display the result in 2d diagram\n",
        "\n"
      ],
      "metadata": {
        "id": "P73OnJkOcLOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## â„¹ï¸ Understanding K-Means Clustering  \n",
        "\n",
        "### Why Clustering? ğŸ§  \n",
        "Once we have numerical representations of articles, we want to **group similar news articles together** based on their content. **K-Means clustering** is a powerful unsupervised learning algorithm that does exactly this!  \n",
        "\n",
        "### How Does K-Means Work? âš™ï¸  \n",
        "1ï¸âƒ£ Choose the number of clusters (**k**)  \n",
        "2ï¸âƒ£ Assign each data point (news article) to the nearest cluster center  \n",
        "3ï¸âƒ£ Recalculate the cluster centers based on assigned points  \n",
        "4ï¸âƒ£ Repeat until cluster assignments stop changing  \n",
        "\n",
        "### Choosing the Right k ğŸ“Š  \n",
        "We use the **Elbow Method** to find the **optimal number of clusters**. This helps prevent **overfitting** (too many clusters) or **underfitting** (too few clusters).  \n",
        "\n",
        "ğŸ¯ **Goal:** Assign each article to a cluster and visualize the grouping! Letâ€™s see if similar news topics naturally emerge. ğŸ”ğŸ“°  \n"
      ],
      "metadata": {
        "id": "wQCjPJLTew7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the optimal number of clusters using Elbow Method\n",
        "\n",
        "\n",
        "\n",
        "# display the result of elbow method\n",
        "\n"
      ],
      "metadata": {
        "id": "IrR9_JWwcS2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply K-Means with optimal k (let's assume 4 based on the Elbow Method)\n",
        "\n",
        "\n",
        "# Visualizing Clusters\n",
        "\n"
      ],
      "metadata": {
        "id": "fz-Hfl8McUKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## â„¹ï¸ Visualizing Clusters with Word Clouds  (Optional Section)\n",
        "\n",
        "### Why Use Word Clouds? ğŸŒ¥ï¸ğŸ”   \n",
        "After clustering the news articles, itâ€™s helpful to **understand the key themes** in each cluster. A **word cloud** is a simple but effective way to visualize the most common words in each group.  \n",
        "\n",
        "### How Does It Work? âš™ï¸  \n",
        "1ï¸âƒ£ We **extract the text** from all articles in a cluster.  \n",
        "2ï¸âƒ£ We **count word frequencies**, giving more importance to frequently occurring words.  \n",
        "3ï¸âƒ£ A **word cloud** is generated, where **larger words** indicate higher frequency in that cluster.  \n",
        "\n",
        "### What Can We Learn? ğŸ¤”  \n",
        "- Identify **dominant keywords** in each cluster.  \n",
        "- Get **insights into topic differences** between clusters.  \n",
        "- Verify if our **K-Means clustering makes sense** based on meaningful word groupings.  \n",
        "\n",
        "ğŸ¯ **Goal:** Use word clouds to quickly interpret the characteristics of each news category! â˜ï¸ğŸ“°ğŸ”  \n"
      ],
      "metadata": {
        "id": "2BTgAHKqe30c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate word cloud\n",
        "\n",
        "\n",
        "# Generate for each cluster\n"
      ],
      "metadata": {
        "id": "oRcP5VpacVrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ§ Critical Thinking & Discussion: Interpreting Our Results ğŸ’¡ğŸ“ŠğŸ“°\n",
        "\n",
        "### 1ï¸âƒ£ PCA Visualization Analysis\n",
        "#### ğŸ” Question:\n",
        "Looking at the PCA Projection of News Articles, we see that the data is spread out but still has some overlapping areas.\n",
        "\n",
        "- What does this distribution tell us about the underlying structure of the articles?\n",
        "- Do you think reducing dimensionality to 2 components sufficiently preserves the key variations in the dataset? Why or why not?\n",
        "\n",
        "\n",
        "### 2ï¸âƒ£ Choosing the Optimal Number of Clusters\n",
        "#### ğŸ“Š Question:\n",
        "The Elbow Method plot suggests a decreasing trend in distortion as k increases.\n",
        "\n",
        "- Based on the curve, what would you choose as the optimal k value?\n",
        "- Why does the distortion decrease as k increases, and why shouldnâ€™t we always choose a very high k?\n",
        "\n",
        "### 3ï¸âƒ£ Interpreting Clusters of News Articles\n",
        "#### ğŸ“° Question:\n",
        "After applying K-Means clustering, the PCA-reduced plot shows four distinct clusters.\n",
        "\n",
        "- How well-separated do the clusters appear? Do they seem meaningful?\n",
        "- What possible themes might each cluster represent in terms of news content? How could we validate our assumptions about these themes?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Iy_gXNQxfQQW"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}